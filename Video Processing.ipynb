{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from aux_functions import *\n",
    "from sklearn.externals import joblib\n",
    "from scipy.ndimage.measurements import label\n",
    "from collections import deque\n",
    "from moviepy.editor import VideoFileClip, ImageSequenceClip\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region of Interest\n",
    "\n",
    "I'll start by defining what area of the images we are going to look for cars in. This will be a horizontal strip where y pixel coordinate would vary between 400 and 600."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ystart = 400\n",
    "ystop = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to classify images so I need to import the classifier that has been previously trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = joblib.load('final_svm.pkl')\n",
    "scaler = joblib.load('final_scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll set in here the scales to use in this case. All other parameters are going to be taken with default values. This is accomplish by default values passed in [aux_functions.py](./aux_functions.py). Those values have been found during the work explained in the other notebooks. I believe this makes it more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scales = [1, 1.5, 2]\n",
    "filtered_boxes = deque()\n",
    "nframe_filter = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function to make ploting shorter\n",
    "def toRGB(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing of the video is quite straightforward in that it follows what has been done for images. The main difference is the use of a fifo queue that keep track of detected boxes. This queue is implemented in the variable filtered_boxes using python's deque.\n",
    "\n",
    "Each frame is pass to process_video. In that function, I search for cars in the frame. The list of boxes were the classifier has detected car are accumulated in the fifo queue. Using this queue to calculate the heat map allows to take into consideration previous frames. This help with eliminating false positives.\n",
    "\n",
    "Another measure that proved very effective in reducing false positives was using the decision function of the classifier instead of the prediction. Thus, in aux_functions.py I set a threshold in the form: test_prediction = classifier.decision_function(test_features) with test_prediction > 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_video(img):\n",
    "    # my code assumes image is read in BGR as it is by opencv. Image from moviepy come as RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    car_boxes = []\n",
    "    heat = np.zeros_like(img[:,:,0]).astype(np.float)\n",
    "    # Take several scales and detect boxes. Add all boxes to a list\n",
    "    # This uses global variables set in this notebook and default values set in aux_functions.py\n",
    "    for scal in scales:\n",
    "        car_boxes += find_cars(img, ystart, ystop, scal, classifier, scaler)\n",
    "    filtered_boxes.appendleft(car_boxes)\n",
    "    if len(filtered_boxes) > nframe_filter:\n",
    "        filtered_boxes.pop()\n",
    "    for boxes in filtered_boxes:\n",
    "        # Add heat to each box in box list\n",
    "        heat = add_heat(heat,boxes)\n",
    "    # Apply threshold to help remove false positives\n",
    "    heat = apply_threshold(heat, nframe_filter)\n",
    "    \n",
    "    # Visualize the heatmap when displaying    \n",
    "    heatmap = np.clip(heat, 0, 255)\n",
    "    # Find final boxes from heatmap using label function\n",
    "    labels = label(heatmap)\n",
    "    draw_img = draw_labeled_bboxes(np.copy(img), labels)\n",
    "    return cv2.cvtColor(draw_img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video ./test_video_processed.mp4\n",
      "[MoviePy] Writing video ./test_video_processed.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 38/39 [00:44<00:01,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: ./test_video_processed.mp4 \n",
      "\n",
      "CPU times: user 5min 2s, sys: 9.28 s, total: 5min 12s\n",
      "Wall time: 45.3 s\n"
     ]
    }
   ],
   "source": [
    "video_output = './test_video_processed.mp4'\n",
    "clip = VideoFileClip('./test_video.mp4')\n",
    "clip_processed = clip.fl_image(process_video)\n",
    "%time clip_processed.write_videofile(video_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"./test_video_processed.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(video_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video ./project_video_processed.mp4\n",
      "[MoviePy] Writing video ./project_video_processed.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1260/1261 [56:31<00:02,  2.58s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: ./project_video_processed.mp4 \n",
      "\n",
      "CPU times: user 4h 50min 57s, sys: 8min 16s, total: 4h 59min 13s\n",
      "Wall time: 56min 32s\n"
     ]
    }
   ],
   "source": [
    "video_output = './project_video_processed.mp4'\n",
    "clip = VideoFileClip('./project_video.mp4')\n",
    "clip_processed = clip.fl_image(process_video)\n",
    "%time clip_processed.write_videofile(video_output, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Results are fairly good. I personally think that my pipeline lacks robustness as it has proven to be quite sensible to tweaking values of scales, thresholds and so on.\n",
    "\n",
    "Another issue seems to be the time is takes to process each frame. To identify bottlenecks, I have run a very simple profiling. As was expected, feature extraction takes quite some time for each frame. Some of it is repeated several times. The first thing that jumps to mind is reducing the time taken by color features extraction. An approach similar to what is done with hog features could be greatly beneficial. This could be done by restricting the scales to multiples of a minimum scale. The features could be calculated at that scale and added in form of cell latter, instead of calculating them again at each scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 23 18:15:22 2017    run_stats\n",
      "\n",
      "         247588 function calls in 2.193 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 84 to 15 due to restriction <15>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        9    0.440    0.049    0.810    0.090 /home/alberto/anaconda3/lib/python3.5/site-packages/skimage/feature/_hog.py:8(hog)\n",
      "     2628    0.396    0.000    0.709    0.000 /home/alberto/anaconda3/lib/python3.5/site-packages/numpy/lib/function_base.py:267(histogram)\n",
      "        9    0.203    0.023    0.203    0.023 {skimage.feature._hoghistogram.hog_histograms}\n",
      "      882    0.198    0.000    0.198    0.000 {cvtColor}\n",
      "    25794    0.164    0.000    0.164    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "    10513    0.056    0.000    0.056    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
      "     2628    0.053    0.000    0.053    0.000 {built-in method numpy.core.multiarray.bincount}\n",
      "     2628    0.049    0.000    0.069    0.000 /home/alberto/anaconda3/lib/python3.5/site-packages/numpy/core/function_base.py:9(linspace)\n",
      "    20532    0.046    0.000    0.201    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
      "      876    0.038    0.000    0.038    0.000 {built-in method numpy.core.multiarray.dot}\n",
      "     7008    0.036    0.000    0.036    0.000 {method 'ravel' of 'numpy.ndarray' objects}\n",
      "      876    0.034    0.000    0.126    0.000 /home/alberto/anaconda3/lib/python3.5/site-packages/sklearn/preprocessing/data.py:633(transform)\n",
      "    20532    0.032    0.000    0.155    0.000 /home/alberto/anaconda3/lib/python3.5/site-packages/numpy/core/_methods.py:31(_sum)\n",
      "        3    0.030    0.010    2.133    0.711 /home/alberto/Programacion/Udacity Self-Driving/CarND-Vehicle-Detection/aux_functions.py:265(find_cars)\n",
      "     3504    0.028    0.000    0.028    0.000 {built-in method numpy.core.multiarray.concatenate}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x7f46720dbcc0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "test_image = cv2.imread('test_images/test4.jpg')\n",
    "cProfile.run('process_video(cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB))', 'run_stats')\n",
    "p = pstats.Stats('run_stats')\n",
    "p.sort_stats('time').print_stats(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
